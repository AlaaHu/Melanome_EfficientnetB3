# -*- coding: utf-8 -*-
"""EfficientnetB3_Melanoma s (FINAL).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/196UR02T6cI4r8y_0c3dvjEV6bxK-TpEm

Same as EfficientNet_Melanoma BUT we used Sigmoid here.

Split:
72% train
8% val
20% test

Main file: HAM10000RTU


EfficientNetB3


0.0001 LR

SGD
"""

!pip install tensorflow

import tensorflow.keras
import numpy as np
import matplotlib.pyplot as plt
import glob
import cv2
from keras import optimizers
from keras.layers import Input, Lambda, Dense, Flatten, Dropout
from keras.models import Model
from tensorflow.keras.applications import EfficientNetB3 #changed
from keras.applications.vgg16 import preprocess_input
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras import metrics
from glob import glob
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import SVC
from sklearn import metrics
import os
from imutils import paths
from cv2 import imread
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.utils import shuffle
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression
import pandas as pd
import seaborn as sns
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

"""# New Section"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

!pip install -q -U keras-tuner

!pip install plot_keras_history

from plot_keras_history import show_history, plot_history

IMAGE_SIZE = [300, 300] #changed

from google.colab import drive
drive.mount('/content/drive')

import glob
import tensorflow as tf
from sklearn import preprocessing



train_images = []
train_labels = []

test_images = []
test_labels = []

val_images = []


for directory_path in glob.glob("/content/drive/MyDrive/HAM10000RTU/train/*"):
    label = os.path.split(directory_path)
    print(label)
    for img_path in glob.glob(os.path.join(directory_path, "*.jpg")):
        print(img_path)
        image=tf.keras.preprocessing.image.load_img(img_path, color_mode='rgb',
        target_size= (300, 300)) #changed
        image=np.array(image)
        train_images.append(image)
        train_labels.append(label)

for directory_path1 in glob.glob("/content/drive/MyDrive/HAM10000RTU/test/*"):
    label1 = os.path.split(directory_path1)
    print(label1)
    for img_path1 in glob.glob(os.path.join(directory_path1, "*.jpg")):
        print(img_path1)
        image1=tf.keras.preprocessing.image.load_img(img_path1, color_mode='rgb',
       target_size= (300, 300)) #changed
        image1=np.array(image1)
        test_images.append(image1)
        test_labels.append(label1)

for directory_path in glob.glob("/content/drive/MyDrive/HAM10000RTU/val/*"):
    for img_path in glob.glob(os.path.join(directory_path, "*.jpg")):
        print(img_path)
        image=tf.keras.preprocessing.image.load_img(img_path, color_mode='rgb',
        target_size= (300, 300)) #changed
        image=np.array(image)
        val_images.append(image)



train_images = np.array(train_images)
train_labels = np.array(train_labels)


test_images = np.array(test_images)
test_labels = np.array(test_labels)

val_images = np.array(val_images)

print(test_labels.shape)
print(train_labels.shape)

print(train_images.shape)
print(test_images.shape)
print(val_images.shape)



EfficientNet = EfficientNetB3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False) #changed

EfficientNet.input

for layer in EfficientNet.layers:
  layer.trainable = False

from tensorflow.keras.layers import BatchNormalization

x = Flatten()(EfficientNet.output)
x = tf.keras.layers.Dropout(0.5)(x)
x=BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001)(x)
x = tf.keras.layers.Dropout(0.5)(x)
x=BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001)(x)
x=Dense(256, kernel_regularizer= tensorflow.keras.regularizers.l2(l= 0.016), activity_regularizer= tensorflow.keras.regularizers.l1(0.006),bias_regularizer= tensorflow.keras.regularizers.l1(0.006), activation= 'relu')(x)
x = tf.keras.layers.Dropout(0.5)(x)
x=Dense(128, kernel_regularizer= tensorflow.keras.regularizers.l2(l= 0.016), activity_regularizer= tensorflow.keras.regularizers.l1(0.006),bias_regularizer= tensorflow.keras.regularizers.l1(0.006), activation= 'relu')(x)
x=Dropout(rate= 0.45, seed= 123)(x)
x=Dense(64,  kernel_regularizer= tensorflow.keras.regularizers.l2(l= 0.016), activity_regularizer= tensorflow.keras.regularizers.l1(0.006),bias_regularizer= tensorflow.keras.regularizers.l1(0.006), activation= 'relu')(x)
prediction = Dense(7,activation='sigmoid')(x)
model = Model(inputs=EfficientNet.input, outputs=prediction)


train_datagen = ImageDataGenerator()
test_datagen = ImageDataGenerator()
val_datagen = ImageDataGenerator()

train_set = train_datagen.flow_from_directory('/content/drive/MyDrive/HAM10000RTU/train',
                                                 target_size = (300, 300), #changed
                                                 batch_size = 32,
                                                 class_mode = 'categorical')

# Create a generator for the test data without augmentation
test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/HAM10000RTU/test',
                                                  target_size=(300, 300), #changed
                                                  batch_size=32,
                                                  class_mode='categorical' )

val_set = val_datagen.flow_from_directory('/content/drive/MyDrive/HAM10000RTU/val',
                                                  target_size=(300, 300), #changed
                                                  batch_size=32,
                                                  class_mode='categorical' )


#SGD OPT
from keras.optimizers import SGD
model.compile(SGD(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])

from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss',verbose=1, patience=20)



import time

#starting time
start = time.time()

batch_size = 32
hist = model.fit(train_set, steps_per_epoch = train_set.samples//batch_size,
                 validation_data = val_set,validation_steps = val_set.samples//batch_size,
                 epochs = 100,callbacks=[es]
                 )
# end time
end = time.time()

# total time taken
print("Execution time of the program is- ", end-start)

# plot the loss
plt.plot(hist.history['loss'], label='train loss',marker='o')
plt.plot(hist.history['val_loss'], label='val loss',marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# plot the accracy
plt.plot(hist.history['accuracy'], label='train accuracy',marker='o')
plt.plot(hist.history['val_accuracy'], label='val accuracy',marker='o')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

train_labels

test_labels

train_labels = train_labels[:, 1]
test_labels = test_labels[:, 1]

label_mapping = {'akiec': 0, 'bcc': 1,'bkl': 2,'df': 3,'mel': 4,'nv': 5,'vasc': 6}
train_labels = np.array([label_mapping[label] for label in train_labels])
test_labels = np.array([label_mapping[label] for label in test_labels])

from sklearn import metrics

# Make predictions on the training data
train_pred_probs = model.predict(train_images)  # Use the original training data
train_pred_labels = np.argmax(train_pred_probs, axis=1)

# Make predictions on the test data
test_pred_probs = model.predict(test_images)
test_pred_labels = np.argmax(test_pred_probs, axis=1)

# Ensure that train_labels and test_labels are correctly shaped
# They should be one-dimensional arrays if not already
train_labels = np.squeeze(train_labels)
test_labels = np.squeeze(test_labels)

# Calculate the accuracy scores
train_accuracy = metrics.accuracy_score(train_labels, train_pred_labels)
test_accuracy = metrics.accuracy_score(test_labels, test_pred_labels)

print("\nTraining Accuracy Score:", train_accuracy * 100)
print("\nTesting Accuracy Score:", test_accuracy * 100)

feat_train=model.predict(train_images)
feat_train=feat_train.reshape(feat_train.shape[0], -1)

feat_test=model.predict(test_images)
feat_test=feat_test.reshape(feat_test.shape[0], -1)

labels = np.array(train_labels)
test_labels=np.array(test_labels)

from sklearn.metrics import accuracy_score

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score


# Create and train the SVM classifier
svm = SVC(gamma='auto', kernel='linear', probability=True)
svm.fit(feat_train, labels)

# Make predictions on the training and test data
predictedtrain = svm.predict(feat_train)
predictedtest = svm.predict(feat_test)

# Calculate and print the training and testing accuracy scores
train_accuracy = accuracy_score(labels, predictedtrain)
test_accuracy = accuracy_score(test_labels, predictedtest)

print("Training Accuracy Score:", train_accuracy* 100)
print("Testing Accuracy Score:", test_accuracy* 100)

cm = confusion_matrix(test_labels, predictedtest)

# Create a beautiful heatmap of the confusion matrix
plt.figure(figsize=(10, 8))
sns.set(font_scale=1.2)  # Adjust font size for labels
sns.heatmap(
    cm, annot=True, fmt='d', cmap='Blues',
    xticklabels=['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vacs'],
    yticklabels=['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vacs'],
    cbar=False
)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()



from sklearn.metrics import precision_score, recall_score, f1_score

# Calculate precision, recall, and F1 score for the test data
precision = precision_score(test_labels, predictedtest, average='weighted')
recall = recall_score(test_labels, predictedtest, average='weighted')
f1 = f1_score(test_labels, predictedtest, average='weighted')

print("Precision:", precision * 100)
print("Recall:", recall * 100)
print("F1 Score:", f1 * 100)

from sklearn.metrics import roc_auc_score

# Calculate the AUC score
# Convert labels to binary format as roc_auc_score does not support multiclass format
binarized_test_labels = LabelBinarizer().fit_transform(test_labels.reshape(-1, 1))
auc = roc_auc_score(binarized_test_labels, svm.predict_proba(feat_test), average='weighted', multi_class='ovo')

print("AUC Score:", auc * 100)

# Calculate specificity for each class
specificity = {}
for i in range(7):  # Assuming 7 classes
    true_negatives = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
    false_positives = np.sum(cm[:, i]) - cm[i, i]
    specificity[i] = true_negatives / (true_negatives + false_positives)

# Print specificity for each class
for i in range(7):
    print(f"Specificity for class {i}: {specificity[i]}")

# Calculate specificity for each class
specificity = {}
class_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vacs']  # Assuming class labels
for label in class_labels:
    i = class_labels.index(label)
    true_negatives = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))
    false_positives = np.sum(cm[:, i]) - cm[i, i]
    specificity[label] = true_negatives / (true_negatives + false_positives)

# Print specificity for each class
for label in class_labels:
    print(f"Specificity for class {label}: {specificity[label]}")

# Calculate average specificity
avg_specificity = np.mean(list(specificity.values()))

print("Average Specificity:", avg_specificity)

from sklearn.metrics import precision_recall_fscore_support
res = []
for l in [0,1,2,3,4,5,6]:
     prec,recall,_,_ = precision_recall_fscore_support(np.array(test_labels)==l,
                                                  np.array(predictedtest)==l,
                                                  pos_label=True,average=None)
     res.append([l,recall[0],recall[1]])

pd.DataFrame(res,columns = ['class','specificity','sensitivity'])

